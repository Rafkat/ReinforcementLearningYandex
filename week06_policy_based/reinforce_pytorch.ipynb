{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:04.192840Z",
     "start_time": "2025-08-25T16:44:04.183479Z"
    }
   },
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !pip install -q gymnasium\n",
    "    !pip install moviepy\n",
    "    !apt install ffmpeg\n",
    "    !pip install imageio-ffmpeg\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:04.488708Z",
     "start_time": "2025-08-25T16:44:04.193745Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:05.339815Z",
     "start_time": "2025-08-25T16:44:05.338019Z"
    }
   },
   "source": [
    "# also you need to install ffmpeg if not installed\n",
    "# for MacOS: ! brew install ffmpeg"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:06.696282Z",
     "start_time": "2025-08-25T16:44:06.391351Z"
    }
   },
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4f8ff45ea0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.037636Z",
     "start_time": "2025-08-25T16:44:08.106268Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T17:03:43.117826Z",
     "start_time": "2025-08-25T17:03:43.114921Z"
    }
   },
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(state_dim[0], 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, n_actions)\n",
    "  #<YOUR CODE: define a neural network that predicts policy logits>\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:45:39.843421Z",
     "start_time": "2025-08-25T16:45:39.841269Z"
    }
   },
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    logits = model(torch.tensor(states).float())\n",
    "    return F.softmax(logits, dim=1).detach().numpy()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:45:40.175412Z",
     "start_time": "2025-08-25T16:45:40.172429Z"
    }
   },
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:46:18.763238Z",
     "start_time": "2025-08-25T16:46:18.760701Z"
    }
   },
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()[0]\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(list(range(n_actions)), p=action_probs)  #<YOUR CODE>\n",
    "\n",
    "        new_s, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:46:20.037116Z",
     "start_time": "2025-08-25T16:46:20.033708Z"
    }
   },
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:52:03.847757Z",
     "start_time": "2025-08-25T16:52:03.841410Z"
    }
   },
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    G = [rewards[-1]]\n",
    "    \n",
    "    for g in rewards[-2::-1]:\n",
    "        G.append(g + gamma * G[-1])\n",
    "    \n",
    "    return G[::-1]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:52:04.281903Z",
     "start_time": "2025-08-25T16:52:04.279192Z"
    }
   },
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T17:04:15.545961Z",
     "start_time": "2025-08-25T17:04:15.541717Z"
    }
   },
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * F.one_hot(actions, env.action_space.n), dim=1)\n",
    "    \n",
    "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -(probs * log_probs).sum(-1).mean()  #<YOUR CODE>\n",
    "    loss = -J_hat - entropy_coef * entropy   #<YOUR CODE>\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T17:04:25.589295Z",
     "start_time": "2025-08-25T17:04:16.176005Z"
    }
   },
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 11.990\n",
      "mean reward: 47.270\n",
      "mean reward: 79.300\n",
      "mean reward: 30.310\n",
      "mean reward: 132.990\n",
      "mean reward: 164.890\n",
      "mean reward: 74.930\n",
      "mean reward: 106.330\n",
      "mean reward: 77.300\n",
      "mean reward: 394.100\n",
      "mean reward: 910.710\n",
      "You Win!\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T17:06:56.126997Z",
     "start_time": "2025-08-25T17:06:53.540736Z"
    }
   },
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with gym.make(\"CartPole-v1\", render_mode=\"rgb_array\") as env, RecordVideo(\n",
    "    env=env, video_folder=\"./videos\"\n",
    ") as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(10)]\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T17:06:56.820653Z",
     "start_time": "2025-08-25T17:06:56.817343Z"
    }
   },
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/rl-video-episode-8.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
